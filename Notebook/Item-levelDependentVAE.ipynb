{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df93709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from torch import nn\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5d8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceNetwork(nn.Module):\n",
    "    \"\"\"Inference Network.\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_sizes,\n",
    "                 activation='softplus', dropout=0.2, label_size=0):\n",
    "        \"\"\"\n",
    "        # TODO: check dropout in main caller\n",
    "        Initialize InferenceNetwork.\n",
    "        Args\n",
    "            input_size : int, dimension of input\n",
    "            output_size : int, dimension of output\n",
    "            hidden_sizes : tuple, length = n_layers\n",
    "            activation : string, 'softplus' or 'relu', default 'softplus'\n",
    "            dropout : float, default 0.2, default 0.2\n",
    "        \"\"\"\n",
    "        super(InferenceNetwork, self).__init__()\n",
    "        assert isinstance(input_size, int), \"input_size must by type int.\"\n",
    "        assert isinstance(output_size, int), \"output_size must be type int.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        if activation == 'softplus':\n",
    "            self.activation = nn.Softplus()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "       \n",
    "        self.hiddens = nn.Sequential(OrderedDict([\n",
    "            ('l_{}'.format(i), nn.Sequential(nn.Linear(h_in, h_out), self.activation))\n",
    "            for i, (h_in, h_out) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:]))]))\n",
    "        \n",
    "        self.f_mu = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.f_mu_batchnorm = nn.BatchNorm1d(output_size, affine=False)\n",
    "        self.f_sigma = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.f_sigma_batchnorm = nn.BatchNorm1d(output_size, affine=False)\n",
    "        self.dropout_enc = nn.Dropout(p=self.dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hiddens(x)\n",
    "        x = self.dropout_enc(x)\n",
    "        mu = self.f_mu_batchnorm(self.f_mu(x))\n",
    "        log_sigma = self.f_sigma_batchnorm(self.f_sigma(x))\n",
    "        return mu, log_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b60f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNetwork(nn.Module):\n",
    "    def __init__(self, device, input_size, emb_size=100, hidden_sizes=(200,200), activation='softplus', dropout=0.2, learn_priors=True, label_size=0):\n",
    "        \"\"\"\n",
    "        Initialize InferenceNetwork.\n",
    "        Args\n",
    "            input_size : int, dimension of input\n",
    "            emb_size : int, dimension of embedding\n",
    "            hidden_sizes : tuple, length = n_layers, (default (200, 200))\n",
    "            activation : string, 'softplus', 'relu', (default 'softplus')\n",
    "        \"\"\"\n",
    "        super(DecoderNetwork, self).__init__()\n",
    "        assert isinstance(input_size, int), \"input_size must by type int.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.inf_net = InferenceNetwork(input_size, emb_size, hidden_sizes, activation)\n",
    "        if label_size > 0:\n",
    "            self.label_classification = nn.Linear(emb_size, label_size)\n",
    "            \n",
    "        # init prior parameters\n",
    "        topic_prior_mean = 0.0\n",
    "        self.prior_mean = torch.tensor(\n",
    "            [topic_prior_mean] * emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.prior_mean = self.prior_mean.to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_mean = nn.Parameter(self.prior_mean)\n",
    "\n",
    "        topic_prior_variance = 1. - (1. / emb_size)\n",
    "        self.prior_variance = torch.tensor(\n",
    "            [topic_prior_variance] * emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.prior_variance = self.prior_variance.to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_variance = nn.Parameter(self.prior_variance)\n",
    "            \n",
    "        self.beta = torch.Tensor(emb_size, input_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.beta = self.beta.to(device)\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        nn.init.xavier_uniform_(self.beta)\n",
    "        self.beta_batchnorm = nn.BatchNorm1d(input_size, affine=False)\n",
    "        \n",
    "        self.drop_theta = nn.Dropout(p=self.dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        \"\"\"Reparameterize the theta distribution.\"\"\"\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def forward(self, x, labels=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # batch_size x n_components\n",
    "        posterior_mu, posterior_log_sigma = self.inf_net(x)\n",
    "        posterior_sigma = torch.exp(posterior_log_sigma)\n",
    "        # generate samples from theta\n",
    "        theta = F.softmax(\n",
    "            self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "        theta = self.drop_theta(theta)\n",
    "\n",
    "        # in: batch_size x input_size x emb_size\n",
    "        recon_x = F.softmax(self.beta_batchnorm(torch.matmul(theta, self.beta)), dim=1)\n",
    "        # recon_x: batch_size x input_size\n",
    "        \n",
    "        # classify labels\n",
    "        estimated_labels = None\n",
    "        if labels is not None:\n",
    "            estimated_labels = self.label_classification(theta)\n",
    "            \n",
    "        return self.prior_mean, self.prior_variance, posterior_mu, posterior_sigma, posterior_log_sigma, recon_x, estimated_labels\n",
    "    \n",
    "    \n",
    "    def get_theta(self, x, labels=None):\n",
    "        with torch.no_grad():\n",
    "            # batch_size x n_components\n",
    "            posterior_mu, posterior_log_sigma = self.inf_net(x)\n",
    "            theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "            return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0f4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, input_size, emb_size=100, hidden_sizes=(200, 200), activation='softplus', dropout=0.2, learn_priors=True, batch_size=64,\n",
    "                 lr=2e-3, momentum=0.99, solver='adam', num_epochs=100, reduce_on_plateau=False,\n",
    "                 num_data_loader_workers=mp.cpu_count(), label_size=0, loss_weights=None, gpu=0):\n",
    "\n",
    "        self.device = (\n",
    "                torch.device(\"cuda:\"+str(gpu))\n",
    "                if torch.cuda.is_available()\n",
    "                else torch.device(\"cpu\")\n",
    "            )\n",
    "        print(\"Device:\", self.device)\n",
    "\n",
    "        assert isinstance(input_size, int) and input_size > 0, \\\n",
    "            \"input_size must by type int > 0.\"\n",
    "        assert isinstance(emb_size, int) and emb_size > 0, \\\n",
    "            \"emb_size must by type int > 0.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "        assert isinstance(learn_priors, bool), \"learn_priors must be boolean.\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \\\n",
    "            \"batch_size must be int > 0.\"\n",
    "        assert lr > 0, \"lr must be > 0.\"\n",
    "        assert isinstance(momentum, float) and 0 < momentum <= 1, \\\n",
    "            \"momentum must be 0 < float <= 1.\"\n",
    "        assert solver in ['adam', 'sgd'], \"solver must be 'adam' or 'sgd'.\"\n",
    "        assert isinstance(reduce_on_plateau, bool), \\\n",
    "            \"reduce_on_plateau must be type bool.\"\n",
    "        assert isinstance(num_data_loader_workers, int) and num_data_loader_workers >= 0, \\\n",
    "            \"num_data_loader_workers must by type int >= 0. set 0 if you are using windows\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.solver = solver\n",
    "        self.num_epochs = num_epochs\n",
    "        self.reduce_on_plateau = reduce_on_plateau\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "\n",
    "        if loss_weights:\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = {\"beta\": 1}\n",
    "            \n",
    "        self.model = DecoderNetwork(self.device, input_size, emb_size, hidden_sizes, activation, dropout, learn_priors=learn_priors, label_size=label_size)\n",
    "\n",
    "        self.early_stopping = None\n",
    "\n",
    "        # init optimizer\n",
    "        if self.solver == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(), lr=lr, betas=(self.momentum, 0.99))\n",
    "        elif self.solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=lr, momentum=self.momentum)\n",
    "\n",
    "        # init lr scheduler\n",
    "        if self.reduce_on_plateau:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10)\n",
    "\n",
    "        # performance attributes\n",
    "        self.best_loss_train = float('inf')\n",
    "\n",
    "        # training attributes\n",
    "        self.model_dir = None\n",
    "        self.nn_epoch = None\n",
    "\n",
    "        # validation attributes\n",
    "        self.validation_data = None\n",
    "\n",
    "        # learned topics\n",
    "        self.best_components = None\n",
    "\n",
    "        # Use cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.USE_CUDA = True\n",
    "        else:\n",
    "            self.USE_CUDA = False\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        \n",
    "    def _loss(self, inputs, recon_x, prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance):\n",
    "\n",
    "        # KL term\n",
    "        # var division term\n",
    "        var_division = torch.sum(posterior_variance / prior_variance, dim=1)\n",
    "        # diff means term\n",
    "        diff_means = prior_mean - posterior_mean\n",
    "        diff_term = torch.sum(\n",
    "            (diff_means * diff_means) / prior_variance, dim=1)\n",
    "        # logvar det division term\n",
    "        logvar_det_division = \\\n",
    "            prior_variance.log().sum() - posterior_log_variance.sum(dim=1)\n",
    "        # combine terms\n",
    "        KL = 0.5 * (\n",
    "            var_division + diff_term - self.emb_size + logvar_det_division)\n",
    "\n",
    "        # Reconstruction term\n",
    "        RL = -torch.sum(inputs * torch.log(recon_x + 1e-10), dim=1)\n",
    "\n",
    "        return KL, RL\n",
    "    \n",
    "    \n",
    "    def _train_epoch(self, loader, epoch):\n",
    "        \"\"\"Train epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        for batch_samples in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            topic_fea = batch_samples['topic_fea']\n",
    "            topic_fea = topic_fea.reshape(topic_fea.shape[0], -1)\n",
    "            \n",
    "            item_fea = batch_samples['item_fea']\n",
    "            item_fea = item_fea.reshape(item_fea.shape[0], -1)\n",
    "\n",
    "            X = torch.cat((topic_fea, item_fea), 1)\n",
    "\n",
    "            if \"labels\" in batch_samples.keys():\n",
    "                labels = batch_samples[\"labels\"]\n",
    "                labels = labels.reshape(labels.shape[0], -1)\n",
    "                labels = labels.to(self.device)\n",
    "            else:\n",
    "                labels = None\n",
    "\n",
    "\n",
    "            X = X.to(self.device)\n",
    "                \n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "            posterior_log_variance, recon_x, estimated_labels = self.model(X, labels)\n",
    "\n",
    "            # backward pass\n",
    "            kl_loss, rl_loss = self._loss(X, recon_x, prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            loss = self.weights[\"beta\"]*kl_loss + rl_loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            if labels is not None:\n",
    "                labels = labels.type(torch.long)\n",
    "                label_loss = torch.nn.MultiLabelSoftMarginLoss()(estimated_labels, labels)\n",
    "                loss += label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X.size()[0]\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, train_loss\n",
    "    \n",
    "    \n",
    "    def fit(self, train_dataset, save_dir=None, verbose=False, patience=5, delta=0,\n",
    "            n_samples=20, model_name=\"vae_model_tmall.pt\"):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        :param train_dataset: PyTorch Dataset class for training data.\n",
    "        :param save_dir: directory to save checkpoint models to.\n",
    "        :param verbose: verbose\n",
    "        :param patience: How long to wait after last time validation loss improved. Default: 5\n",
    "        :param delta: Minimum change in the monitored quantity to qualify as an improvement. Default: 0\n",
    "        :param n_samples: int, number of samples of the document topic distribution (default: 20)\n",
    "\n",
    "        \"\"\"\n",
    "        # Print settings to output file\n",
    "        if verbose:\n",
    "            print(\"Settings: \\n\\\n",
    "                   N Components: {}\\n\\\n",
    "                   Topic Prior Mean: {}\\n\\\n",
    "                   Topic Prior Variance: {}\\n\\\n",
    "                   Hidden Sizes: {}\\n\\\n",
    "                   Activation: {}\\n\\\n",
    "                   Dropout: {}\\n\\\n",
    "                   Learn Priors: {}\\n\\\n",
    "                   Learning Rate: {}\\n\\\n",
    "                   Momentum: {}\\n\\\n",
    "                   Reduce On Plateau: {}\\n\\\n",
    "                   Save Dir: {}\".format(\n",
    "                self.emb_size, 0.0,\n",
    "                1. - (1. / self.emb_size), self.hidden_sizes, self.activation, self.dropout, self.learn_priors,\n",
    "                self.lr, self.momentum, self.reduce_on_plateau, save_dir))\n",
    "\n",
    "        self.model_dir = save_dir\n",
    "        train_data = train_dataset\n",
    "#         num_workers=self.num_data_loader_workers,\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        # init training variables\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        # train loop\n",
    "#         pbar = tqdm(self.num_epochs, position=0, leave=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.nn_epoch = epoch\n",
    "            # train epoch\n",
    "            s = datetime.datetime.now()\n",
    "            sp, train_loss = self._train_epoch(train_loader, epoch)\n",
    "            samples_processed += sp\n",
    "            e = datetime.datetime.now()\n",
    "#             pbar.update(1)\n",
    "\n",
    "            if save_dir is not None:\n",
    "                self.save(save_dir)\n",
    "\n",
    "#             pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tTime: {}\".format(\n",
    "#                 epoch + 1, self.num_epochs, samples_processed,\n",
    "#                 len(train_data) * self.num_epochs, train_loss, e - s))\n",
    "            \n",
    "            torch.save(model, model_name)\n",
    "\n",
    "        pbar.close()\n",
    "        \n",
    "\n",
    "    def get_embeddings(self, dataset, batch_size=32, n_samples=20):\n",
    "        self.model.eval()\n",
    "#         num_workers=self.num_data_loader_workers\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_data_loader_workers)\n",
    "        pbar = tqdm(n_samples, position=0, leave=True)\n",
    "        final_thetas = []\n",
    "        for sample_index in range(n_samples):\n",
    "            with torch.no_grad():\n",
    "                collect_theta = []\n",
    "                for batch_samples in loader:\n",
    "                    # batch_size x vocab_size\n",
    "                    topic_fea = batch_samples['topic_fea']\n",
    "                    topic_fea = topic_fea.reshape(topic_fea.shape[0], -1)\n",
    "\n",
    "                    item_fea = batch_samples['item_fea']\n",
    "                    item_fea = item_fea.reshape(item_fea.shape[0], -1)\n",
    "\n",
    "                    X = torch.cat((topic_fea, item_fea), 1)\n",
    "                    \n",
    "                    labels = None\n",
    "\n",
    "                    X = X.to(self.device)\n",
    "\n",
    "                    # forward pass\n",
    "                    self.model.zero_grad()\n",
    "                    theta = self.model.get_theta(X, labels).cpu().numpy()\n",
    "                    collect_theta.extend(theta.tolist())\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
    "                final_thetas.append(np.array(collect_theta))\n",
    "        pbar.close()\n",
    "        return np.sum(final_thetas, axis=0) / n_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc697084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.sparse\n",
    "class ModelDataset(Dataset):\n",
    "    \"\"\"Class to load user and topic features.\"\"\"\n",
    "    def __init__(self, n_topics, n_items, topic, items, labels=None):\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        self.n_items = n_items\n",
    "        self.topic = topic\n",
    "        self.items = items\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.topic)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return sample from dataset at index i.\"\"\"\n",
    "            \n",
    "        topic_hot = torch.zeros(self.n_topics)\n",
    "        topic_hot[self.topic[i]] = 1 \n",
    "\n",
    "        item_hot = torch.zeros(self.n_items)\n",
    "        for item in self.items[i]:\n",
    "            item_hot[item] = 1\n",
    "            \n",
    "        return_dict = {'topic_fea': topic_hot, 'item_fea': item_hot}\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            labels = self.labels[i]\n",
    "            return_dict[\"labels\"] = torch.FloatTensor(labels)\n",
    "            \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f133f01",
   "metadata": {},
   "source": [
    "# New rocket retail data 20/09/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7770c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../datasets/RocketRetail/rocket.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce398db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>past_topic</th>\n",
       "      <th>future_topic</th>\n",
       "      <th>past_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[760]</td>\n",
       "      <td>[760]</td>\n",
       "      <td>[[69481]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[268, 23]</td>\n",
       "      <td>[310, 800, 1086, 760, 819, 312, 813, 543, 790,...</td>\n",
       "      <td>[[537], [66499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[713]</td>\n",
       "      <td>[713]</td>\n",
       "      <td>[[73498]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[790]</td>\n",
       "      <td>[1009]</td>\n",
       "      <td>[[84857, 84857, 84857]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[918]</td>\n",
       "      <td>[918]</td>\n",
       "      <td>[[16129]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234557</th>\n",
       "      <td>[405]</td>\n",
       "      <td>[473]</td>\n",
       "      <td>[[28074]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234558</th>\n",
       "      <td>[281]</td>\n",
       "      <td>[426]</td>\n",
       "      <td>[[32825]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234559</th>\n",
       "      <td>[332]</td>\n",
       "      <td>[332]</td>\n",
       "      <td>[[73003]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234560</th>\n",
       "      <td>[325]</td>\n",
       "      <td>[1056]</td>\n",
       "      <td>[[86658]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234561</th>\n",
       "      <td>[760]</td>\n",
       "      <td>[760]</td>\n",
       "      <td>[[80928]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234562 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       past_topic                                       future_topic  \\\n",
       "0           [760]                                              [760]   \n",
       "1       [268, 23]  [310, 800, 1086, 760, 819, 312, 813, 543, 790,...   \n",
       "2           [713]                                              [713]   \n",
       "3           [790]                                             [1009]   \n",
       "4           [918]                                              [918]   \n",
       "...           ...                                                ...   \n",
       "234557      [405]                                              [473]   \n",
       "234558      [281]                                              [426]   \n",
       "234559      [332]                                              [332]   \n",
       "234560      [325]                                             [1056]   \n",
       "234561      [760]                                              [760]   \n",
       "\n",
       "                      past_leaf  \n",
       "0                     [[69481]]  \n",
       "1              [[537], [66499]]  \n",
       "2                     [[73498]]  \n",
       "3       [[84857, 84857, 84857]]  \n",
       "4                     [[16129]]  \n",
       "...                         ...  \n",
       "234557                [[28074]]  \n",
       "234558                [[32825]]  \n",
       "234559                [[73003]]  \n",
       "234560                [[86658]]  \n",
       "234561                [[80928]]  \n",
       "\n",
       "[234562 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3394cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_topics = set()\n",
    "unique_items = set()\n",
    "\n",
    "topics = []\n",
    "items = []\n",
    "\n",
    "for past_topic, future_topic, past_leaf in df.values:\n",
    "    unique_topics.update(past_topic)\n",
    "    unique_topics.update(future_topic)\n",
    "    \n",
    "    topics.extend(past_topic)\n",
    "    \n",
    "    for pl in past_leaf:\n",
    "        unique_items.update(pl)\n",
    "        items.append(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f000282",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = len(unique_topics)\n",
    "n_items = len(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5628aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ModelDataset(n_topics, n_items, topics, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a94c2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_size=n_topics+n_items, emb_size=256, batch_size=256, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4fedcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1270/1270 [06:09<00:00,  3.44it/s]\n",
      "Epoch 2: 100%|██████████| 1270/1270 [06:07<00:00,  3.45it/s]\n",
      "Epoch 3: 100%|██████████| 1270/1270 [06:12<00:00,  3.41it/s]\n",
      "Epoch 4: 100%|██████████| 1270/1270 [06:08<00:00,  3.44it/s]\n",
      "Epoch 5: 100%|██████████| 1270/1270 [06:14<00:00,  3.40it/s]\n",
      "Epoch 6: 100%|██████████| 1270/1270 [06:14<00:00,  3.39it/s]\n",
      "Epoch 7: 100%|██████████| 1270/1270 [06:12<00:00,  3.41it/s]\n",
      "Epoch 8: 100%|██████████| 1270/1270 [06:13<00:00,  3.40it/s]\n",
      "Epoch 9: 100%|██████████| 1270/1270 [06:43<00:00,  3.14it/s]\n",
      "Epoch 10: 100%|██████████| 1270/1270 [06:24<00:00,  3.31it/s]\n",
      "Epoch 11: 100%|██████████| 1270/1270 [06:38<00:00,  3.19it/s]\n",
      "Epoch 12: 100%|██████████| 1270/1270 [06:28<00:00,  3.27it/s]\n",
      "Epoch 13: 100%|██████████| 1270/1270 [06:59<00:00,  3.03it/s]\n",
      "Epoch 14: 100%|██████████| 1270/1270 [07:00<00:00,  3.02it/s]\n",
      "Epoch 15: 100%|██████████| 1270/1270 [06:59<00:00,  3.03it/s]\n",
      "Epoch 16: 100%|██████████| 1270/1270 [06:55<00:00,  3.05it/s]\n",
      "Epoch 17: 100%|██████████| 1270/1270 [06:04<00:00,  3.49it/s]\n",
      "Epoch 18: 100%|██████████| 1270/1270 [06:13<00:00,  3.40it/s]\n",
      "Epoch 19: 100%|██████████| 1270/1270 [06:11<00:00,  3.41it/s]\n",
      "Epoch 20: 100%|██████████| 1270/1270 [06:12<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea78bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../datasets/RocketRetail/vae_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567642e0",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "778bc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../datasets/RocketRetail/vae_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "emb = model.get_embeddings(dataset, n_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d61fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_items_to_emb = {}\n",
    "i = 0\n",
    "for top, it in zip(topics, items):\n",
    "    _id = tuple([top, tuple(it)])\n",
    "    topic_items_to_emb[_id] = emb[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_items_to_emb-new.pkl',\"wb\") as fp:\n",
    "    pickle.dump(topic_items_to_emb, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594694f9",
   "metadata": {},
   "source": [
    "# Tmall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ea9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../datasets/Tmall/tmall.pickle\", 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2955a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_topics = set()\n",
    "unique_items = set()\n",
    "\n",
    "topics = []\n",
    "items = []\n",
    "\n",
    "for past_topic, future_topic, past_leaf in data.values:\n",
    "    unique_topics.update(past_topic)\n",
    "    unique_topics.update(future_topic)\n",
    "    \n",
    "    topics.extend(past_topic)\n",
    "    \n",
    "    for pl in past_leaf:\n",
    "        unique_items.update(pl)\n",
    "        items.append(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78af446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_idx = {}\n",
    "idx_to_item = {}\n",
    "for i, item in enumerate(unique_items):\n",
    "    item_to_idx[item] = i\n",
    "    idx_to_item = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012188f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Tmall/item_to_idx.pkl', 'wb') as fp:\n",
    "    pickle.dump(item_to_idx, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0483c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Tmall/idx_to_item.pkl', 'wb') as fp:\n",
    "    pickle.dump(idx_to_item, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde8309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [[item_to_idx[i] for i in its] for its in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f637292",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics, n_items = len(unique_topics), len(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ef785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ModelDataset(n_topics, n_items, topics[:], items[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08d99962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_size=n_topics+n_items, emb_size=256, batch_size=256, num_epochs=20, gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████| 7374/7374 [51:50<00:00,  2.37it/s]\n",
      "Epoch 2: 100%|██████████████████████████████| 7374/7374 [51:36<00:00,  2.38it/s]\n",
      "Epoch 3: 100%|██████████████████████████████| 7374/7374 [59:21<00:00,  2.07it/s]\n",
      "Epoch 4: 100%|████████████████████████████| 7374/7374 [1:00:21<00:00,  2.04it/s]\n",
      "Epoch 5: 100%|██████████████████████████████| 7374/7374 [52:54<00:00,  2.32it/s]\n",
      "Epoch 6: 100%|██████████████████████████████| 7374/7374 [51:51<00:00,  2.37it/s]\n",
      "Epoch 7: 100%|██████████████████████████████| 7374/7374 [52:28<00:00,  2.34it/s]\n",
      "Epoch 8: 100%|██████████████████████████████| 7374/7374 [52:22<00:00,  2.35it/s]\n",
      "Epoch 9: 100%|██████████████████████████████| 7374/7374 [51:56<00:00,  2.37it/s]\n",
      "Epoch 10:  81%|███████████████████████▌     | 5995/7374 [43:12<10:07,  2.27it/s]"
     ]
    }
   ],
   "source": [
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../datasets/Tmall/vae_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a47acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
